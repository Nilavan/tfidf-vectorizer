{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "# Custom implementation of tfidf vectorizer\n",
    "\n",
    "Note:\n",
    "- Corpus - Sample list of sentences.\n",
    "- Corpus data - Cleaned strings data from [file](cleaned_strings).\n",
    "\n",
    "## Task 1: \n",
    "Building a Tfidf vectorizer and comparing the results with Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjuCcJwXxLjJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "def idf(dataset, word):\n",
    "    '''function to find the idf of a word'''\n",
    "    count = 0 # number of documents with the word in it, initially 0\n",
    "    \n",
    "    # for each review in dataset and if word is in the review increment count\n",
    "    for row in dataset:\n",
    "        if word in row:\n",
    "            count += 1\n",
    "    \n",
    "    # calculate idf as 1+log((1+total number of documents)/(1+number of documents with the word))\n",
    "    val = 1 + math.log((1+len(dataset))/(1+count)) # value of idf \n",
    "    return val\n",
    "\n",
    "def fit(dataset):\n",
    "    unique_words = set() # set of unique words at first empty\n",
    "    if isinstance(dataset, list): # check if its list type\n",
    "        \n",
    "        # for each review in dataset and for each word in review add word to set\n",
    "        for row in dataset:\n",
    "            for words in row.split():\n",
    "                if len(words) < 2: # ignore small words \n",
    "                    continue\n",
    "                unique_words.add(words)\n",
    "                \n",
    "        # change type set to list and sort\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        \n",
    "        # vocabulary of words and index {unique word : index of word}\n",
    "        vocab = {i:j for j,i in enumerate(unique_words)}\n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"Pass a list as sentence\")\n",
    "        \n",
    "def transform(dataset, vocab):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, list): # check if its list type\n",
    "        for idx, row in enumerate(tqdm(dataset)): # for each document in dataset\n",
    "\n",
    "            word_freq = dict(Counter(row.split())) # key: word, values: frequency of word\n",
    "            doc_total_freq = len(row.split()) # total number of words in document\n",
    "            \n",
    "            for word, freq in word_freq.items(): # for each item in word_freq dictionary\n",
    "                \n",
    "                if len(word) < 2: # ignore small words \n",
    "                    continue\n",
    "                        \n",
    "                col_index = vocab.get(word, -1) # retreving the dimension number of a word\n",
    "                # if the word exists\n",
    "                if col_index != -1:\n",
    "                    rows.append(idx) # store index of document\n",
    "                    columns.append(col_index) # store dimensions of word\n",
    "                    \n",
    "                    # calculate tfidf value using formula\n",
    "                    values.append((freq/doc_total_freq)*idf(dataset,word)) # store tfidf value of word\n",
    "        return normalize(csr_matrix((values, (rows,columns)), shape = (len(dataset),len(vocab))), norm ='l2') # final output normalized \n",
    "    else:\n",
    "        print(\"Pass list of strings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Using corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 8710.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom feature names:\n",
      " ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "Custom idf values:\n",
      " [1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n",
      "\n",
      "Custom tfidf output:\n",
      "   (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "  (1, 1)\t0.6876235979836937\n",
      "  (1, 3)\t0.2810886740337529\n",
      "  (1, 5)\t0.5386476208856762\n",
      "  (1, 6)\t0.2810886740337529\n",
      "  (1, 8)\t0.2810886740337529\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.4697913855799205\n",
      "  (3, 2)\t0.580285823684436\n",
      "  (3, 3)\t0.3840852409148149\n",
      "  (3, 6)\t0.3840852409148149\n",
      "  (3, 8)\t0.3840852409148149\n",
      "\n",
      "Dense matrix for document 0:\n",
      " [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "\n",
    "vocab = fit(corpus)\n",
    "print('\\nCustom feature names:\\n',list(vocab.keys()))\n",
    "custom_idf = [idf(corpus,word) for word in list(vocab.keys())]\n",
    "print('\\nCustom idf values:\\n',custom_idf)\n",
    "custom_tfidf = transform(corpus, vocab)\n",
    "print('\\nCustom tfidf output:\\n',custom_tfidf)\n",
    "\n",
    "# sparse matrix into dense matrix\n",
    "print('\\nDense matrix for document 0:\\n',custom_tfidf[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sklearn feature names:\n",
      " ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "Sklearn idf values:\n",
      " [1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n",
      "\n",
      "Sklearn output:\n",
      "   (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 5)\t0.5386476208856763\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (3, 8)\t0.38408524091481483\n",
      "  (3, 6)\t0.38408524091481483\n",
      "  (3, 3)\t0.38408524091481483\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn implementation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)\n",
    "\n",
    "print('\\nSklearn feature names:\\n',vectorizer.get_feature_names())\n",
    "\n",
    "print('\\nSklearn idf values:\\n',vectorizer.idf_)\n",
    "\n",
    "print('\\nSklearn output:\\n',skl_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using cleaned strings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:00<00:00, 1404.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom output:\n",
      "   (0, 53)\t0.4285381075435814\n",
      "  (0, 688)\t0.4285381075435814\n",
      "  (0, 720)\t0.4285381075435814\n",
      "  (0, 1545)\t0.22805947499067006\n",
      "  (0, 1651)\t0.1625302544476425\n",
      "  (0, 1653)\t0.3509903637504196\n",
      "  (0, 2287)\t0.342724587634745\n",
      "  (0, 2878)\t0.3605325019883844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus_data = pickle.load(f)\n",
    "\n",
    "vocab = fit(corpus_data)\n",
    "custom_tfidf_data = transform(corpus_data, vocab)\n",
    "print('\\nCustom output:\\n',custom_tfidf_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2878)\t0.35781145622317734\n",
      "  (0, 2287)\t0.3377679916467555\n",
      "  (0, 1653)\t0.35781145622317734\n",
      "  (0, 1651)\t0.16192317905848022\n",
      "  (0, 1545)\t0.30566026894803877\n",
      "  (0, 720)\t0.4123943870778812\n",
      "  (0, 688)\t0.4123943870778812\n",
      "  (0, 53)\t0.4123943870778812\n"
     ]
    }
   ],
   "source": [
    "# using sklearn\n",
    "vectorizer_data = TfidfVectorizer()\n",
    "vectorizer_data.fit(corpus_data)\n",
    "skl_output_data = vectorizer_data.transform(corpus_data)\n",
    "print(skl_output_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMxBmVZExLjK"
   },
   "source": [
    "## Task 2:\n",
    "Implementing max features functionality, i.e, limiting to 50 terms with top idf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_DJnnR3xLjR"
   },
   "outputs": [],
   "source": [
    "def idf(dataset, word):\n",
    "    '''function to find the idf of a word'''\n",
    "    count = 0 # number of documents with the word in it, initially 0\n",
    "    \n",
    "    # for each review in dataset and if word is in the review increment count\n",
    "    for row in dataset:\n",
    "        if word in row:\n",
    "            count += 1\n",
    "    \n",
    "    # calculate idf as 1+log((1+total number of documents)/(1+number of documents with the word))\n",
    "    val = 1 + math.log((1+len(dataset))/(1+count)) # value of idf \n",
    "    return val\n",
    "        \n",
    "def fit_50(dataset):\n",
    "    '''function to fit 50 terms with top idf scores'''\n",
    "    unique_words = set() # set of unique words at first empty\n",
    "    if isinstance(dataset, list): # check if its list type\n",
    "        \n",
    "        # for each review in dataset and for each word in review add word to set\n",
    "        for row in dataset:\n",
    "            for words in row.split():\n",
    "                if len(words) < 2: # ignore small words \n",
    "                    continue\n",
    "                unique_words.add(words)\n",
    "                \n",
    "        # change type set to list\n",
    "        unique_words = list(unique_words)\n",
    "        \n",
    "        # v = {word : idf of word}\n",
    "        v = {word:idf(dataset,word) for word in unique_words}\n",
    "        \n",
    "        # the two lines below is to check top 50 sorted words by idf \n",
    "        # sorted_v = sorted(v,key = v.get,reverse=True)[:50]\n",
    "        # print(sorted_v)\n",
    "\n",
    "        # vocabulary of top 50 words by idf and index, from v sorted by idf {unique word : index of word}\n",
    "        vocab = {word:i for i,word in enumerate(sorted(v,key=v.get,reverse=True)[:50])}\n",
    "\n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"Pass a list as sentence\")\n",
    "        \n",
    "def transform(dataset, vocab):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, list): # check if its list type\n",
    "        for idx, row in enumerate(tqdm(dataset)): # for each document in dataset\n",
    "            word_freq = dict(Counter(row.split())) # key: word, values: frequency of word\n",
    "            doc_total_freq = len(row.split()) # total number of words in document\n",
    "            \n",
    "            for word, freq in word_freq.items(): # for each item in word_freq dictionary\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                        \n",
    "                col_index = vocab.get(word, -1) # retreving the dimension number of a word\n",
    "                # if the word exists\n",
    "                if col_index != -1:\n",
    "                    rows.append(idx) # store index of document\n",
    "                    columns.append(col_index) # store dimensions of word\n",
    "                    \n",
    "                    # calculate tfidf value using formula\n",
    "                    values.append((freq/doc_total_freq)*idf(dataset,word)) # store tfidf value of word\n",
    "\n",
    "        return normalize(csr_matrix((values, (rows,columns)), shape = (len(dataset),len(vocab))), norm ='l2') # final output normalized \n",
    "    else:\n",
    "        print(\"Pass list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:00<00:00, 79091.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words in sorted vocab:\n",
      " ['sheer', 'versus', 'juano', 'paolo', 'behind', 'shenanigans', 'stowe', 'contract', 'sacrifice', 'reasonable', 'shallow', 'renowned', 'gay', 'painted', 'ireland', 'avoided', 'trooper', 'aesthetically', 'subplots', 'gake', 'ursula', 'reminded', 'exemplars', 'hype', 'assaulted', 'shattered', 'excuses', 'incomprehensible', 'monica', 'roller', 'rubbish', 'zombiez', 'suggests', 'regrettable', 'repair', 'defensemen', 'detailing', 'alongside', 'reporter', 'boyle', 'says', 'smoothly', 'holds', 'stuff', 'wide', 'changing', 'nerves', 'judge', 'borrowed', 'notch']\n",
      "\n",
      "Idf values:\n",
      " [('sheer', 6.922918004572872), ('versus', 6.922918004572872), ('juano', 6.922918004572872), ('paolo', 6.922918004572872), ('behind', 6.922918004572872), ('shenanigans', 6.922918004572872), ('stowe', 6.922918004572872), ('contract', 6.922918004572872), ('sacrifice', 6.922918004572872), ('reasonable', 6.922918004572872), ('shallow', 6.922918004572872), ('renowned', 6.922918004572872), ('gay', 6.922918004572872), ('painted', 6.922918004572872), ('ireland', 6.922918004572872), ('avoided', 6.922918004572872), ('trooper', 6.922918004572872), ('aesthetically', 6.922918004572872), ('subplots', 6.922918004572872), ('gake', 6.922918004572872), ('ursula', 6.922918004572872), ('reminded', 6.922918004572872), ('exemplars', 6.922918004572872), ('hype', 6.922918004572872), ('assaulted', 6.922918004572872), ('shattered', 6.922918004572872), ('excuses', 6.922918004572872), ('incomprehensible', 6.922918004572872), ('monica', 6.922918004572872), ('roller', 6.922918004572872), ('rubbish', 6.922918004572872), ('zombiez', 6.922918004572872), ('suggests', 6.922918004572872), ('regrettable', 6.922918004572872), ('repair', 6.922918004572872), ('defensemen', 6.922918004572872), ('detailing', 6.922918004572872), ('alongside', 6.922918004572872), ('reporter', 6.922918004572872), ('boyle', 6.922918004572872), ('says', 6.922918004572872), ('smoothly', 6.922918004572872), ('holds', 6.922918004572872), ('stuff', 6.922918004572872), ('wide', 6.922918004572872), ('changing', 6.922918004572872), ('nerves', 6.922918004572872), ('judge', 6.922918004572872), ('borrowed', 6.922918004572872), ('notch', 6.922918004572872)]\n",
      "\n",
      "Final tfidf output\n",
      "   (19, 29)\t0.5773502691896258\n",
      "  (19, 45)\t0.5773502691896258\n",
      "  (19, 46)\t0.5773502691896258\n",
      "  (37, 20)\t1.0\n",
      "  (41, 10)\t1.0\n",
      "  (104, 13)\t1.0\n",
      "  (108, 25)\t1.0\n",
      "  (109, 31)\t1.0\n",
      "  (121, 9)\t1.0\n",
      "  (129, 11)\t1.0\n",
      "  (135, 36)\t0.7071067811865476\n",
      "  (135, 47)\t0.7071067811865476\n",
      "  (143, 34)\t1.0\n",
      "  (148, 16)\t0.5773502691896257\n",
      "  (148, 35)\t0.5773502691896257\n",
      "  (148, 44)\t0.5773502691896257\n",
      "  (169, 14)\t1.0\n",
      "  (181, 26)\t1.0\n",
      "  (185, 22)\t1.0\n",
      "  (200, 28)\t1.0\n",
      "  (261, 7)\t1.0\n",
      "  (289, 24)\t1.0\n",
      "  (295, 4)\t1.0\n",
      "  (372, 15)\t1.0\n",
      "  (381, 48)\t1.0\n",
      "  (411, 30)\t1.0\n",
      "  (421, 1)\t0.8944271909999159\n",
      "  (421, 8)\t0.4472135954999579\n",
      "  (422, 32)\t1.0\n",
      "  (430, 37)\t1.0\n",
      "  (436, 5)\t1.0\n",
      "  (542, 42)\t1.0\n",
      "  (562, 43)\t1.0\n",
      "  (618, 49)\t1.0\n",
      "  (628, 2)\t1.0\n",
      "  (632, 21)\t1.0\n",
      "  (643, 18)\t1.0\n",
      "  (644, 0)\t0.5\n",
      "  (644, 38)\t0.5\n",
      "  (644, 39)\t0.5\n",
      "  (644, 40)\t0.5\n",
      "  (652, 27)\t1.0\n",
      "  (667, 17)\t1.0\n",
      "  (672, 23)\t1.0\n",
      "  (674, 3)\t1.0\n",
      "  (696, 6)\t1.0\n",
      "  (714, 19)\t1.0\n",
      "  (734, 12)\t1.0\n",
      "  (735, 41)\t1.0\n",
      "  (742, 33)\t1.0\n",
      "\n",
      "Final tfidf output for document 421\n",
      "   (0, 1)\t0.8944271909999159\n",
      "  (0, 8)\t0.4472135954999579\n",
      "\n",
      "Dense matrix for document 421\n",
      " [[0.         0.89442719 0.         0.         0.         0.\n",
      "  0.         0.         0.4472136  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "\n",
      "Shape:  (1, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus_data = pickle.load(f)\n",
    "\n",
    "vocab_50 = fit_50(corpus_data)\n",
    "print('\\nWords in sorted vocab:\\n',list(vocab_50.keys()))\n",
    "custom_idf_data_50 = [(word,idf(corpus_data,word)) for word in list(vocab_50.keys())]\n",
    "print('\\nIdf values:\\n',custom_idf_data_50)\n",
    "custom_tfidf_data_50 = transform(corpus_data, vocab_50)\n",
    "print('\\nFinal tfidf output\\n',custom_tfidf_data_50)\n",
    "print('\\nFinal tfidf output for document 421\\n',custom_tfidf_data_50[421])\n",
    "\n",
    "print('\\nDense matrix for document 421\\n',custom_tfidf_data_50[421].toarray())\n",
    "print('\\nShape: ',custom_tfidf_data_50[421].toarray().shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
